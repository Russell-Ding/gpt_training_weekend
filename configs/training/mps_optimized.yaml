# @package training

# Optimizer configuration (flattened for direct access)
lr: 6e-4                           # Learning rate (GPT-3 optimal)
beta1: 0.9                         # Adam beta1
beta2: 0.95                        # Î²2=0.95 is crucial for LLMs!
eps: 1e-8                          # Adam epsilon
weight_decay: 0.1                  # High weight decay for regularization

# Learning rate scheduling (SGDR - Stochastic Gradient Descent with Restarts)
warmup_steps: 2000                 # Linear warmup
max_steps: 100000                  # Total training steps
min_lr_ratio: 0.1                  # Minimum LR as fraction of max LR

# Training loop configuration
batch_size: 8                       # Base batch size (memory constrained)
gradient_accumulation_steps: 8      # Effective batch size = 8 * 8 = 64
eval_interval: 1000                 # Steps between evaluations
save_interval: 5000                 # Steps between checkpoints
log_interval: 100                   # Steps between logging
eval_batch_size: 4                  # Smaller batch for validation

# Gradient management
grad_clip: 1.0                      # Gradient clipping norm

# Memory optimization
gradient_checkpointing: false       # Gradient checkpointing (trades compute for memory)
mixed_precision: true               # Use FP16 for forward/backward, FP32 for optimizer