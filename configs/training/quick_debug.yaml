# @package training

# Fast iteration configuration for debugging
optimizer:
  _target_: torch.optim.AdamW
  lr: 1e-3                          # Higher LR for faster convergence
  betas: [0.9, 0.95]
  weight_decay: 0.01                # Lower weight decay

scheduler:
  type: "constant"                  # No scheduling for simplicity

# Minimal training for quick iteration
batch_size: 4
gradient_accumulation_steps: 2      # Effective batch size = 8
max_steps: 1000                     # Very short training
eval_interval: 100                  # Frequent evaluation
save_interval: 500                  # Frequent checkpoints
log_interval: 10                    # Frequent logging

# Relaxed MPS settings for stability
mps_settings:
  max_memory_fraction: 0.5          # Conservative memory usage
  amp_dtype: "float32"              # Disable mixed precision for debugging

gradient:
  clip_norm: 5.0                    # Relaxed gradient clipping

validation:
  max_eval_steps: 10                # Minimal validation

metadata:
  description: "Quick debug configuration for fast iteration"
  purpose: "debugging_and_development"