# @package _global_

# Default configuration composition
defaults:
  - model: tiny_gpt              # Which model config to use
  - training: mps_optimized       # Which training config to use
  - data: shakespeare             # Which dataset config to use
  - _self_                        # Include this file's settings

# Global experiment settings
experiment_name: "gpt_weekend_project"
seed: 42
debug: false
resume_from_checkpoint: null  # Path to checkpoint to resume from (null for new training)

# Output directory configuration
output_dir: "outputs/${experiment_name}/${now:%Y-%m-%d_%H-%M-%S}"
checkpoint_dir: "${output_dir}/checkpoints"
log_dir: "${output_dir}/logs"

# Weights & Biases configuration
wandb:
  enabled: false                  # Set to true to enable W&B logging
  project: "gpt-training-weekend"
  name: null                      # Run name (auto-generated if null)
  run_id: null                    # Resume specific run (null for new run)
  watch_model: false              # Watch model gradients (expensive)

# System configuration
system:
  device: "mps"                  # "auto", "mps", "cuda", or "cpu"
  num_workers: 0                  # DataLoader workers (0 for MPS compatibility)
  pin_memory: false               # Not needed for unified memory
  mixed_precision: true           # Use mixed precision training

# Hydra configuration (controls Hydra's behavior)
hydra:
  # Output directory for Hydra's files
  run:
    dir: "${output_dir}"

  # Sweep configuration for hyperparameter optimization
  sweep:
    dir: "outputs/sweeps/${now:%Y-%m-%d_%H-%M-%S}"
    subdir: "${hydra.job.num}"

  # Job configuration
  job:
    chdir: false                  # Don't change working directory